\documentclass[12pt]{article}

\setlength\parindent{0pt}
\newcommand{\myt}[1]{\textbf{\underline{#1}}}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}

\title{\vspace{-15ex}CS370: Interpolation \vspace{-1ex}}
\date{January 11th, 2017}
\author{Graham Cooper}

\begin{document}
	\maketitle
	
	See figure 2.1\\
	
	$$y = p(x)$$
	We want to find a function p, such that the curve is 'nice' (where nice is \underline{piecewise polynomial} or \underline{polynomial})\\
	
	Given:\\
	$(x_1, y_1), (x_2,y_2)...,(x_n, y_n)$ n points $ x_1 < x_2 < ... < x_n$\\
	
	Find a polynomial P(x) of degree $< n$\\
	In general:\\
	$$p(x) = c_1 + c_2x + c_3x^2 + ... + c_nx^{n-1}$$
	$$p(x_1) = y_1$$
	$$p(x_2) = y_2$$
	$$...$$
	$$p(x_n) = y_n$$
	
	n unknowns, n equations (linear)\\
	
	\myt{Example:}\\
	$(-1, 1), (1,1), (2,5), (4,1)$\\
	See figure 2.2\\
	
	$$p(x) = c_1 + c_2x + c_3x^2 + c_4x^3$$
	$$p(-1) = c_1 - c_2 + c_3 - c_4 = 1$$
	$$p(1) = c_1 + c_2 + c_3 + c_4 = 1$$
	$$p(2) = c_1 + 2c_2 + 4c_3 + 8c_4 = 5$$
	$$p(4) = c_1 + 4c_2 + 16c_3 + 64c_4 = 1$$
	
	$
	\begin{Bmatrix}
		1 & -1 & 1 & -1 & | & 1 \\
		1 & 1 & 1 & 1 & | & 1 \\
		1 & 2 & 4 & 8 & | & 5 \\
		1 & 4 & 16 & 64 & | & 1
	\end{Bmatrix}
	$//
	Solve the matrix!!\\
	
	Now we are just writing out the solution...\\
	$$p(x) = c_1 + c_2x + c_3x^2 + c_4x^3$$
	$$= 1 + b_2(x-1)+b_3(x-1)^2+b_4(x-1)^3$$
	$$= L_1(x) + L_2(x) + 5L_3(x) + L_4(x)$$
	
	$$L_1(x) = \frac{(x-1)(x-2)(x-4)}{-30}$$
	$$L_2(x) = \frac{(x+1)(x-2)(x-4)}{6}$$
	$$L_3(x) = \frac{(x+1)(x-1)(x-4)}{-6}$$
	$$L_4(x) = \frac{(x+1)(x-1)(x-2)}{30}$$
	
	I think we are writing it out this way so that we can easily plug in the vlues and get the correct points??
	
	Question:\\
	\begin{enumerate}
		\item Does an interpolating polynomial always exist?
		\item If (1) is true then is the answer always unique?
	\end{enumerate}

	$$p(x) = c_1 + c_2x + ... c_nx^{n-1}$$
	$$p(x_1) = c_1 + c_2x_1 + ... c_nx_1^{n-1}$$
	$$p(x_2) = c_1 + c_2x_2 + ... c_nx_2^{n-1}$$
	$$...$$
	$$p(x_n) = c_1 + c_2x_n + ... c_nx_n^{n-1}$$
	
	$
	\begin{Bmatrix}
		1 & x_1 & x_1^2 & ... & x_1^{n-1} \\
		1 & x_2 & x_2^2 & ... & x_2^{n-1} \\
		... & ... & ... & ... & ... \\
		1 & x_n & x_n^2 & ... & x_n^{n-1} \\
	\end{Bmatrix}
	\begin{Bmatrix}
		c_1 \\ c_2 \\ ... \\ c_n
	\end{Bmatrix}
	 = 
	 \begin{Bmatrix}
		 y_1 \\ y_2 \\ ... \\ y_n
	 \end{Bmatrix}
	$
	
	The first matrix is the vandermonde (V) matrix.\\
	V is invertable, $V \times \overrightarrow{c} = \overrightarrow{y}$\\
	det $V \ne 0$ and det $V = \pi(x_i - x_j) \ne 0$ for $i < j$\\
	
	Remember what a determinate is, remember what invertible is, but we will never be asked to do it.\\
	
	$p(x)$
	$$p(x) = q_1(x)(x-x_1) + y_1$$
	$$p(x) = q_2(x)(x-x_2) + y_2$$
	$$...$$
	$$p(x) = q_n(x)(x-x_n) + y_n$$
	
	\subsection*{Lagrange Polynomial}
	
	$$(x_1, y_1), (x_2,y_2)...(x_n,y_n)$$
	$$p(x) = y_1L_1(x) + y_2L_2(x) + ... + y_nL_n(x)$$
	
	$L_i(x_i) = 1, L_i(x_j) = 0$ for $i \ne j$ and $deg(L_i) = n-1$\\
	Lets construct $L_1$ using the above\\
	$$L_1(x) = \frac{(x-x_2)(x-x_3)...(x-x_n)}{(x_1-x_2)(x_1-x_3)...(x_1-x_n)}$$
	
	$$L_i(x) = \frac{(x-x_1)...(x-x_{i-1})(x-x_{i+1})...(x-x_n)}{(x_i-x_1)...(x_i-x_{i-1})...(x_i-x_n)}$$
	$L_i(x_i) = 1$ and $L_j(x_j) = 0$ where $j \ne i$\\
	
	For A1 Q3 (January 13th) - figuring out the solution to the recurrence - and using the answer to help\\
	$$?? \boxed{I_n} \leftarrow I_{n-1} \leftarrow I_{n-2} \leftarrow ... \leftarrow I_0$$
	$$\checkmark \boxed{\hat{I_n}} \leftarrow \hat{I_{n-1}} \leftarrow ... \leftarrow \hat{I_1} \leftarrow \hat{I_0}$$
	$$e_n \leftarrow e_{n-1} \leftarrow ... \leftarrow e_1 \leftarrow e_0$$
	$$e_n = (-\alpha)^ne_0$$
	$$I_n ?= formula(I_0) = $$
	
	Using p?\\
	$$?? \boxed{p_n} \leftarrow p_{n-1} p_{n-2}, p_{n-2}p_{n-3}, ..., p_1, p_0$$
	$p_n = as^n + bt^n$ and $a,b$ depend on $p_0, p_1$\\
	$$\checkmark \boxed{\hat{p_n}} \leftarrow \hat{p_{n-1}} \hat{p_{n-2}} ..., \hat{p_1}\hat{p_0}$$
	This line but with hats (I got lazy)  $p_n = as^n + bt^n$ and $a,b$ depend on $p_0, p_1$\\
	solve for $e_n$\\
	
	Recall from Jan 11th: (regoing over the start of this page)\\
	
	\subsection*{Lagrange Form (again)}
	For $x_1, x_2, ... x_n$ distinct, construct $L_1(x), L_2(x)...L_n(x)$\\
	Satisfying:\\
	\begin{enumerate}
		\item $L_i(x)$ has degree n-1
		\item $L_i(x_i) = 1$
		\item $L_i(x_j) = 0$ if $i \ne j$
	\end{enumerate}

	How do we construct this:\\
	$$L_1(x) = \frac{(x-x_2)(x-x_3)...(x-x_n)}{(x_1-x_2)(x_1-x_3)...(x_1-x_n)}$$
	We divide like this in order to get an equation that satisfies that if we plug in $x_1$ we will end up getting 1 as required, otherwise we will be getting a 0. This is actually pretty cool. Neat!\\
	
	$$L_i(x) = \frac{(x-x_1)...(x-x_{i-i})(x-x_{i+1})...(x-x_n)}{(x_i-x_1)...(x_i-x_{i-1})(x_i-x_{i+1})...(x_i-x_n)}$$
	
	$$p(x) = y_1L_1(x) + y_2L_2(x) + ... + y_nL_n(x)$$
	$$p(x_1) = y_11 + y_20 + ... + y_n0 = y_1$$
	$$...$$
	$$p(x_n) = y_10 + y_20 + ... + y_n1 = y_n$$
	
	A question that he often has asked on midterms ( and is almost 100\% going to add it to ours):\\
	Given: $x_1,x_2,x_3x_4$ as $-1,1,2,117,412$\\
	Form $p(x) = L_1(x) + L_2(x) + L_3(x) + L_4(x)$\\
	Write $p(x) = c_1 + c_2x + c_3x^2 + c_4x^3$\\
	
	Draw the graph!\\
	Solve for the 4 numbers, and find what is y at each of the 4 points?\\
	Then we find out that f(x) = 1 for each\\
	Therefore the solution is p(x) = 1\\
	
	\subsection*{Cubic Hermite Interpolation}
	Another type of interpolation\\
	
	Given: $(x_L, y_L)$ more on the left side and $(x_R, y_R)$ on the right side, $S_L$ slope of the left side, and $S_R$ the slope of the right side\\
	
	$p(x)$ has degree at most 3 since we have 4 uknowns\\
	$p(x_L) = y_L$, $p(x_R) = y_R$, $p'(x_L) S_L$, $p'(x_R) = S_R$\\
	
	$p(x) = c_1 + c_2(x-x_L) + c_3(x-x_L)^2 + c_4(x-x_L)^3$
	$p'(x) = c_2 + 2c_3(x-x_L) + 3c_4(x-x_L)^2$
	$p(x_L) = y_L \implies c_1$
	$p'(x_L) = S_L \implies c_2$
	$p(x_R) = y_R \implies c_1 + c_2\Delta x + c_3\Delta x^2 + c_4 \Delta x^3 = y_R$
	$p'(x_R) = S_R \implies c_2 + 2c_3\Delta x + 3c_4\Delta x^2 = S_R$
	where $\Delta x = x_R - x_L$
	
	$
	\begin{Bmatrix}
	 1 & 0 & 0 & 0 & | & Y_L \\
	 0 & 1 & 0 & 0 & | & S_L \\
	 1 & \Delta x & \Delta x^2 & \Delta x^3 & | & Y_R \\
	 0 & 1 & 2\Delta x & 3\Delta x^2 & | & S_L \\
	\end{Bmatrix}
	$\\
	becomes\\ 	
	$
	\begin{Bmatrix}
	1 & 0 & 0 & 0 & | & Y_L \\
	0 & 1 & 0 & 0 & | & S_L \\
	0 & 0 & 1 & 0 & | & \frac{3Y_R' - 2S_L - S_R}{\Delta x} \\
	0 & 0 & 0 & 1 & | & \frac{S_R + S_L - 2y_L'}{\Delta x^2} \\
	\end{Bmatrix}
	$
	
	$$c_1 = y_L$$
	$$c_2 = S_L$$
	$$c_3 = \frac{3Y_R' - 2S_L - S_R}{\Delta x}$$
	$$c_4 = \frac{S_R + S_L - 2y_L'}{\Delta x^2}$$
	
	Sub into p(x)\\
	$$p(x) = 3 - (x-1) + 3(x-1)^2 - (x-1)^3$$
	
	From Jan 16th:\\
	See image Interp1.1: He is showing that the polynomial (red line) could be bad, we want the green line instead.\\
	
	\subsection*{Cubic Spline}
	Given: $(x_1,y_1),...,(x_N,y_N)$ N points\\
	$x_1 < x_2 < ... < x_{N-1} < x_{N}$\\
	
	A \underline{cubic spline} is a function S(x) defined on the interval $[x_1, x_N]$ which satisfies the following:\\
	(see interp1.2 figure)
	\begin{enumerate}
		\item In each interval $[x_i,x_{i+1}]$ S(x) is a cubic polynomial. $S_i(x) = a_i + b_i(x-x_i) + c_i(x-x_i)^2 + d_i(x-x_i)^3$
		\item S(x) interpolates the N points: $S(x_i) = y_i$
		\item S'(x) is continuous
		\item S''(x) is continuous
		\item 2 other things??
	\end{enumerate}
	\myt{Is this well defined?}\\
	How man unknowns? 4 per interval, N-1 intervals $\rightarrow 4N-4$ unknowns\\
	How many conditions (equations)?\\
	Condition(2) $\rightarrow$ 2 equations per interval $\rightarrow 2N-2$\\
	$S_i(x_i) = y_i$ and $S_i(x_{i+1}) = y_{i+1}$\\
	Condition(3) - 1 equation per interior point $\rightarrow N-2$\\
	Condition(4) - 1 equation per interiour point $\rightarrow N-2$\\
	In total we get $4N-6$ equations\\
	
	\myt{Boundary Conditions}
	\begin{enumerate}
		\item Natural cubic spline $S''(x_1) = 0$, $S''(x_N) = 0$
		\item Clamped cubic spline $S'(x_1) = s_1$ and $S'(x_N) = s_N$ $s_1, s_N$ are known
		\item Periodic Cubic spline $S'(x_N) = S'(x_1)$ and $S''(x_N) = S''(x_1)$
		\item Not-a-knot condition (Matlab default) $S'''(x)$ is continuous at $x_2$ and $x_{N-1}$
	\end{enumerate}

	\subsubsection*{How do we compute a cubic spline?}
	\myt{Method 1}:\\
	Have $4N-4$ uknowns and $4N-4$ linear equations $\rightarrow$ sove via Gaussian elimination\\
	This is a cost of: $O((4N-4)^3) = O(N^3)$\\
	
	\myt{Method 2}:\\
	Think of the deriviatives $S_1, S_2, ... S_N$ as the unknowns. We will set up linear equations for these derivatives\\
	Then:\\
	\begin{enumerate}
		\item This will give us $S_1(x),S_2(x), ..., S_{N-1}(x)$
		\item We will solve linear system in O(N) operations
	\end{enumerate}

	Given: (-2,1), (0,0), (1,3), (4,-1), (5,2)\\
	Clamped\\
	(Figure Interp1.3)\\
	
	S(x) = \\
	$$a_1 + b_1(x+2) + c_!(x+2)^2 + d_1(x+2)^3 \_for\_ -2 \le x \le 0$$
	$$a_2 + b_2x + c_2x^2 + d_2x^3 \_for\_ 0 \le x \le 1$$
	$$a_3 + b_3(x-1) + c_3(x-1)^2 + d_3(x-1)^3 \_for\_ 1 \le x \le 4$$
	$$a_4 + b_4(x-4) + c_4(x-4)^2 + d_4(x-4)^3 \_for\_ 4 \le x \le 5$$
	
	16 uknowns and 16 equations\\
	$a_1 = 1$, $a_2 = 0$, $a_3 = 3$, $a_4 = -1$\\
	$a_1 + 2b_1 + 4c_1 + 8d_1 = 0$\\
	$a_2 + b_2 + c_2 + d_2 = 3$\\
	$\checkmark$\\
	$\checkmark$\\
	$b_1 + 4c_1 + 12d_1 = b_2$\\
	$2c_1 + 12d_1 = 2c_2$\\
	$\checkmark$\\
	$\checkmark$\\
	$\checkmark$\\
	$\checkmark$\\
	$b_1 = 1$\\
	$b_4 = 2c_4 + 3d_4 = 0$\\
	
	Go and do the assignment question that looks like this\\
	
	\myt{Generic example:}\\
	Given: $(x_1, y_1), (x_2,y_2)..., (x_N, y_N)$\\
	let $s_1, s_2, s_3 ... s_N$ denote the derivitive values of the spline S(x) at the points. These are uknowns, but they exist.\\
	Figure interp1.4\\
	figure interp1.5 is a blown up of $X_i$\\
	
	$$S_i(x) = a_i + b_i(x-x_i) + c_i(x-x_i)^2 + d_i(x-x_i)^3$$
	$a_i = y_i$\\
	$b_i = s_i$\\
	$c_i = \frac{3y_i' - 2s_i - s_{i+1}}{\Delta x_i}$\\
	$d_i = \frac{s_i + s_{i+1} - 2y_i'}{\Delta x_i^2}$\\
	$\Delta x_i = x_{i+1} - x_i$\\
	$Y_i' = \frac{y_{i+1} - y_i}{x_{i+1}-x_i}$\\
	
	Additional information:\\
	$$S''(x_1) = 0, S_1''(x_2) = S_2''(x_2), S_2''(x_3) = S_3''(x_3), ... S_{N-2}''(x_{N-1}) = S_{N-1}''(X_{N-1}), S_{N-1}''(x_N) = 0$$
	
	\myt{Equation 1:}\\
	$2c_1 = 0$ ie. $\frac{3y_1' - 2s_1 - s_2}{\Delta x_1} = 0$\\
	$2s_1 + s_2 = 3y_1'$\\
	
	We now want to set up a linear set of equations for the esses\\
	
	$
	\begin{Bmatrix}
	* & * \\
	... \\
	... \\
	... \\
	... \\
	* & *
	\end{Bmatrix}
	\begin{Bmatrix}
	s_1 \\
	s_2 \\
	s_3 \\
	... \\
	s_{n-1}\\
	s_N\\
	\end{Bmatrix}
	=
	\begin{Bmatrix}
	*  \\
	... \\
	... \\
	... \\
	... \\
	*
	\end{Bmatrix}
	$
	
	$$c_{N-1} + 3d_{N-1}(x_N - x_{N-1}) = 0$$
	$$3Y_{N-1}' - 2S{N-1} - S_N) + 3(S_{N-1} + S_N - 2Y_{N-1}') = 0$$
	He deleted things on the bottom :(((\\
	$$S_{N-1} + 2S_{N} = 3Y_{N-1}'$$
	
	$$(x_i) = S_i''(x_i)$$
	For $i = 2,3,...N-1$\\
	$$= 2c_i$$
	$$= \frac{2(3Y_i' - 2s_i - s_{i+1})}{\Delta x_i}$$
	
	$S_{i-1}''(x) = 2c_{i-1} + 6d_{i-1}(x-x_{i-1})$\\
	$S_{i-1}(x) = a_{i-1} + b_{i-1}(x-x_i) + c_{i-1}(x0x_{i-1})^2 + d_{i-1}(x-x_{i-1})^3$\\
	
	$c_{i-1} + 3d_{i-1}\Delta x_{i-1}$\\
	$$\Delta x_i((3Y_{i-1}' - 2s_{i-1}-s_{i}) + 3(s_{i-1} + s_i - 2y_{i-1}')) = (3Y_i' - 2s_i - s_{i-1})\Delta x_{i-1}$$
	$$\Delta x_iS_{i-1} + 2(\Delta x_i + \Delta x_{i-1})S_i$$
	$$\Delta x_{i-1}S_{i+1} = 3y_{i-1}'\Delta x_i + 3y_i'\Delta x_{i-1}$$
	
	The above is assignment question 5, but like wtf is going on...\\
	Object is to find values for the S's in the matrix...\\
	
	\myt{From Jan 20th}\\
	
	See slides.
	
	\myt{From Jan 25th}\\
	
	Slide: Forward Euler Example \#1:\\
	
	a):\\
	
	$$y_{n+1} = y_n + hf(t_r,y_n)$$
	$$y_{n+1} = y_{n} + (1) \times 2y_n$$
	$$y_{n+1} = 3y_n$$
	
	First equation\\
	\begin{tabular}{c | c c c }
		n & $t_n$ & $y_n$ & $y(t_n)$\\ \hline
		0 & 1 & 3 & 3\\
		1 & 2 & 9 & 22\\
		2 & 3 & 27 & 164\\
		3 & 4 & 81 & 1210\\
		4 & 5 & 243 & 8943\\
	\end{tabular}


	Second equation\\
	\begin{tabular}{c | c c c }
		n & $t_n$ & $y_n$ & $y(t_n)$\\ \hline
		0 & 1 & 3 & 3\\
		1 & 1.5 & 6 & 8\\
		2 & 2 & 12 & 22\\
		3 & 2.5 & 81 & 60\\
		4 & 3 & 243 & 163\\
		5 & 3.5 & 96 & 445\\
		6 & 4 & 192 & 1210 \\
	\end{tabular}

	Third Equation\\
	\begin{tabular}{c | c c c }
		n & $t_n$ & $y_n$ & $y(t_n)$\\ \hline
		0 & 1 & 3 & 3\\
		1 & 2 & 9 & 22\\
		2 & 3 & 27 & 164\\
		3 & 4 & 81 & 1210\\
		4 & 5 & 243 & 8943\\
	\end{tabular}

	\myt{Example 2}\\
	a)\\
	$y_{n+1} = y_n + hf(t_n,y_n)$\\
	$
	\begin{bmatrix}
		X_{n+1} \\
		y_{n+1}
	\end{bmatrix}
	=
	\begin{bmatrix}
	 X_n \\
	 y_n \\
	\end{bmatrix}
	+ 2 \times
	\begin{bmatrix}
		-y_n \\
		x_n \\
	\end{bmatrix}
	$\\
	
	\begin{tabular}{c | c c c }
		n & $t_n$ & $x_n$ & $y_n$\\ \hline
		0 & 0 & 2 & 0\\
		1 & 2 & 2 & 4\\
		2 & 4 & -6 & 8\\
		3 & 6 & -22 & -4\\
	\end{tabular}

	$x_1 = x_0 - 2y_0 = 2-2(0) = 2$\\
	$y_1 = y_0 + 2x_0 = 0 + 2(2) = 4$\\
	$x_2 = x_1 - 2y_1 = 2 - 2(4) = -6$\\
	$y_2 = y_1 + 2x_1 = 4 + 2(2) = 8$\\
	
	\subsection*{Deriving Forward Euler}
	
	\myt{Method 1}\\
	Approximate y' with "finite differences":\\
	0DE: $y'(t) = f(t,y(t))$\\
	Approximate: $y'(t_n) \approx \frac{y_{n+1} - y_n}{h} = f(t_n,y_n)$\\
	$\rightarrow y_{n+1} = y_n + hf(t_n, y_n)$\\
	
	\myt{Method 2}\\
	Use a Taylor series to estimate $y(t_{n+1})$ and drop high order terms.\\
	$$y(t_{n+1}) = y(t_n + h) \approx y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + O(h^3)$$
	Remove terms due to estimations
	$$y(t_{n+1}) = y(t_n + h) \approx y(t_n) + hy'(t_n)$$
	$$y_{n+1} = y_n + hf(t_n, y_n)$$
	
	\myt{Jan 27th}\\
	
	\subsection*{Forward Euler Error}
	Slide Understanding Forward Euler:\\
	
	FE is $y_{n+1} = y_n + hf(t_n, y_n)$\\
	Taylor series is: $y(t_{n+1}) = y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + O(h^3)$\\
	The error is the difference, assuming exact data, at $t_n$ ie $y_n = y(t_n)$\\
	
	So FE becomes:
	$$y_{n+1} = y(t_n) + hf(t_n,y(t_n)) = y(t_n) + hy'(t_n)$$
	The difference is:\\
	$$y_{n+1} - y(t_n) = \frac{-h^2}{2}y''(t_n) + O(h^3)$$
	$$= O(h^2)$$
	The local truncation error of FE is $O(h^2)$. Error decreases quadratically with h (the time step)\\
	
	\subsection*{Deriving Trapezoidal Rule}
	Keeping more terms slide\\
	
	Taylor series is: $y(t_{n+1}) = y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + O(h^3)$\\
	Replace the y'' term with finit differences approximation:\\
	$y(t_{n+1}) = y(t_n) + hy'(t_n) + \frac{h^2}{2}[\frac{y'(t_{n+1} - y'(t_n))}{g} + O(h)](t_n) + O(h^3)$\\
	The muliploicatino by $h^2$ gets rid of the O(h)\\
	
	Simplify:\\
	$$y(t_{n+1}) = y(t_n) + hy'(t_n) + \frac{h}{2}y'(t_{n+1}) - \frac{h}{2}y'(t_n) + O(h^3)$$
	$$= y(t_n) + \frac{h}{2}[f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1}))] + O(h^3)$$
	$\therefore$ Trapezoidal is $O(h^3)$:\\
	$$y_{n+1} = y_n + \frac{h}{2}[f(t_n, y_n) + f(t_{n+1}, y_{n+1})]$$
	
	\subsection*{Deriving Modified Euler}
	Error of improved euler slide:\\
	Trapezoidal: $y_{n+1} = y_n + \frac{h}{2}[f(t_n, y_n) + f(t_{n+1}, y_{n+1})]$\\
	ForwardEuler: $y(t_{n+1}) = y(t_n) + hy'(t_n) + O(h^2)$\\
	Let $y*_{n+1} = y(t_n) + hf(t_n,y(t_n))$ so 1st step has error:\\
	$y(t_{n+1}) - y*_n+1 = O(h^2)$\\
	
	Taylor expanding f, we get:\\
	$$f(t_{n+1}, y(t_{n+1})) = f(t_{n+1}, y*_{n+1}) + \frac{2f}{2y}(t_{n+1},y*_{n+1})(y(t_{n+1}) - y*_{n+1}) + O((y(t_{n+1}) - y*_{n+1})^2)$$
	$$\therefore f(t_{n+1}, y(t_{n+1})) = f(t_{n+1}, y*_{n+1}) + O(h^2)$$
	Plugging into trapezoidal:\\
	$$y(t_{n+1}) = y(t_n) + \frac{h}{2}[f(t_n, y(t_n)) + f(t_{n+1}, y*_{n+1}) + O(h^2)] + O(h^3)$$
	$$= y(t_n) + \frac{h}{2}[f(t_n,y(t_n) + f(t_{n+1},y*_{n+1})] + O(h^3)$$
	
	\subsection*{Improved Euler Example}
	Apply Improved Euler with h = 2 for 2 steps:\\
	General Form: $y*_{n+1} = y_n + hf(t_n, y_n)$\\
	$y_{n+1} y_n \frac{h}{2}[f(t_n, y_n) + f(t_{n+1},y*_{n+1})]$\\
	$IVR = X'(t) = -y(t)$ and $y'(t) = x(t)$\\
	$\therefore$ we have\\
	$$X^t_{n+1} = x_n + 2(-y_n)$$
	$$y^t_{n+1} = y_n + 2(x_n)$$
	$$x_{n+1} = x_n + \frac{2}{2}[-y_n + -y*_{n+1}]$$
	$$y_{n+1} = y_n + \frac{2}{2}[X_n + X*_{n+1}]$$
	
	Step through\\
	Step1 (n = 0)\\
	$$x_1* = X_0 - 2y_0 = 2-2(0) = 2$$
	$$y_1* = y_0 + 2X_0 = 0 + 2(2) = 4$$
	$$x_1 = x_0 - y_0 - y_1* = 2-0-4 = -2$$
	$$y_1 = y_0 + x_0 + x_1* = 0 + 2 + 2 = 4$$

	\begin{tabular}{ c | c c c | c c}
		n & $t_n$ & $X_n$ & $y_n$ & $X*_n$ & $Y*_n$ \\ \hline
		0 & 0 & 2 & 0 & X & X \\
		1 & 2 & -2 & 4 & 2 & 4 \\
		2 & 4 & -6 & -8 & -10 & 0\\
	\end{tabular}

	\subsection*{Trapezoidal Example}
	Generic for: $y_{n+1} = y_n + \frac{h}{2}[f(t_n,y_n) + f(t_{n+1}, y_{n+1})]$\\
	For this problem with h = 2\\
	$$X_{n+1} = X_n + \frac{2}{2}[-y_n - y_{n+1}]$$
	$$y_{n+1} = y_n + \frac{2}{2}[x_n + x_{n+1}]$$
	
	$$X_{n+1} + y_{n+1} = X_n + y_n$$
	$$-X_{n+1} + y_{n+1} = X_n + y_n$$
	OR\\
	$
	\begin{bmatrix}
	1 & 1 \\
	-1 & 1 \\
	\end{bmatrix}
	\begin{bmatrix}
	x_{n+1} \\
	y_{n+1} \\
	\end{bmatrix}
	=
	\begin{bmatrix}
	x_n - y_n \\
	x_n + y_n \\
	\end{bmatrix}
	$
	
	Slide form Deriving BDF1/BDF2\\
	
	Fit lagrange polnomial to $(t_n, y_n)$ and $(t_{n+1}, y_{n+1})$\\
	$$p(t) = y_n(\frac{t - t_{n+1}}{t_n - t_{n+1}}) + y_{n+1}(\frac{t-t_n}{t_{n+1}-t_n})$$
	$$p(t) = y_n(\frac{t-t_{n+1}}{-h}) + y_{n+1}(\frac{t-t_n}{h})$$
	$$p'(t) = \frac{y_{n+1} - y_n}{h}$$
	This gives the slope which rerequire to match f at the end of step time\\
	$$p'(t_{n+1}) = \frac{y_{n+1}-y_n}{h} = f(t_{n+1}, y_{n+1})$$
	
	Be is:\\
	$$y_{n+1} y_n + hf(t_{n-1}, y_{n-1})$$
	\subsection*{Stability}
	\subsubsection*{Stability of Forward Euler}
	F.E. is $Y_{n+1} = y_n + hf(t_n,y_n)$\\
	Test Equation: is $y'(t) = -\lambda y(t)$ for $\lambda > 0$\\
	Applying FE to this problem:\\
	$$y_{n+1} = y_n + h(-\lambda y_n)$$
	$$y_{n+1} = y_n(1-h\lambda)$$
	
	Close form is: $y_n = y_0(1-h\lambda)^n$\\
	True solution is: $y(t) = y_0e^{-\lambda t}$ which decays to 0 as $t \rightarrow \infty$\\
	
	Forward Euler goes to 0 when...\\
	$|1-h\lambda| < 1$ or $-1 < 1-h\lambda < 1$\\\\
	
	Left size says $-1 < 1 - h\lambda$ or $-2 < -h\lambda$ or $h < \frac{2}{\lambda}$\\
	Right side says\\
	$1 - h\lambda < 1$ or $-h\lambda < 0$ or $h > 0$\\
	Hence numerical solution decays to 0\\
	for $0 < h < \frac{2}{\lambda}$ Otherwise it "blows up"\\
	
	If we perturb the initial condiiton by some error $\epsilon_0$, how does it propagate?\\
	$$y_0^{(p)} = y_0 + \epsilon_0 \rightarrow y_n^{(p)} = (y_0 + \epsilon_0)(1-h\lambda)^n$$
	Hence error is $\epsilon_n = y_n^{(0)} - y_n = \epsilon_0(1-h\lambda)^n$\\
	$\therefore$ error follows the same behaviour blows up for $h > \frac{2}{\lambda}$
	
	Forward Euler is "conditionally stable" ie. stable when the stability ocndition is satisfied.\\
	
	\subsubsection*{Stability of Backwards Euler}
	BE is $y_{n+1} = y_n + hf(t_{n+1}, y_{n+1})$\\
	Plug in test equation:\\
	$$y_{n+1} = y_n + h(-\lambda y_{n+1})$$
	$$y_{n+1} = \frac{y_n}{(1+h\lambda)}$$
	Closed form:
	$$y_n = \frac{y_0}{(1+h\lambda)^n}$$
	
	Perturbing $y_0$ by $\epsilon_0$ as before gives:\\
	$$\epsilon_n = \frac{\epsilon_0}{(1+h\lambda)^n}$$
	We need $|1 + h\lambda| > 1$, but since $h > 0$ and $\lambda > 0$ error decreases for any h\\
	
	In general, implicit schemes are more stable than explicit. Usually this stability comes at the cost of more expensive calculations per step.\\
	
	Note: Stability does not imply accuracy! Large steps will still cause large (truncation) error, even if a scheme is unconditionally stable.\\
	
	\subsubsection*{Stability of Improved Euler}
	IE is $y_{n+1}* = y_n + hf(t_n,y_n)$\\
	$y_{n+1} = y_n + \frac{h}{2}[f(t_n, y_n) + f(t_{n+1}, y_{n+1}*)]$\\
	Plug in Test equation ($y'(t) = -\lambda y(t)$ for $\lambda > 0$\\
	1: $y_{n+1}* = y_n + h(-\lambda y_n)$\\
	2: $y_{n+1} = y_n - \frac{h}{2}\lambda y_n - \frac{h\lambda}{2}y_{n+1}*$\\
	
	Plug 1 into 2:\\
	$$y_{n+1} = y_n - \frac{h\lambda}{2}y_n - \frac{h\lambda}{2}(y_n - h\lambda y_n)$$
	$$y_{n+1} = y_n - h\lambda y_n + \frac{h^2\lambda^2}{2}y_n$$
	$$y_{n+1} = y_n(1-h\lambda + \frac{h^2\lambda^2}{2})$$
	
	Again the error behaves likewise so closed for mis $\epsilon_n = \epsilon_0(1-h\lambda + \frac{h^2\lambda^2}{2})^n$\\
	
	Therefore stable when:\\
	$$|1-h\lambda + \frac{h^2\lambda^2}{2} | < 1$$
	ie:\\
	$$-1 < 1-h\lambda+ \frac{h^2\lambda^2}{2} < 1$$
	After some algebra:\\
	$$0 < h < \frac{2}{\lambda}$$
	ie the same conditional stability as FE\\
	
	\myt{From Jan 6th}\\
	
	\subsection*{Process For finding truncation error}
	\subsubsection*{Taylor Series Reminder}
	$$f(x) = f(a) + (x-a)f'(a) + \frac{(x-a)^2f''(a)}{2!} + \frac{(x-a)^3f'''(a)}{3!} ...$$
	
	\subsubsection*{Truncation error of Trapezoidal}
	\begin{enumerate}
		\item Replace RHS data with exact values:\\
		 $y_{n+1} = y_n + \frac{h}{2}(f(t_n,y_n) + f(t_{n+1}, y_{n+1})$\\
		 $\rightarrow$ $y_{n+1} = y(t_n) + \frac{h}{2}y'(t_n) + \frac{h}{2}y'(t_{n+1})$
		\item Taylor series expand RHS Quantities about time $t_n$:\\
		$y'(t_{n+1}) = y'(t_n) + hy''(t_n) + \frac{h^2}{2}y'''(t_n) + O(h^3)$\\
		Plug Back in:\\
		$y_{n+1} = y(t_n) + \frac{h}{2}[h'(t_n) + y'(t_n) + hy''(t_n) + \frac{h^2}{2}y'''(t_n) + O(h^3)]$\\
		$= y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + \frac{h^3}{4}y'''(t_n) + O(h^4)$
		\item Exact taylor series is:\\
		$y(t_{n+1}) = y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + \frac{h^3}{6}y'''(t_n) + O(h^4)$
		\item The Difference is:\\
		$LTE = y(t_{n+1}) - y_{n+1} = h^3[\frac{1}{6} - \frac{1}{4}]y'''(t_n) + O(h^4)$\\
		$= \frac{-1}{12}h^3y'''(t_n) + O(h^4)$\\
		$= O(h^3)$
	\end{enumerate}
	
	\subsubsection{Truncation of BDF2}
	$y_{n+1} = \frac{4}{3}y_n - \frac{1}{3}y_{n-1} + \frac{2}{3}hf(t_{n+1}, y_{n+1})$\\
	\begin{enumerate}
		\item Replace RHS data with exact data\\
		$y_{n+1} = \frac{4}{3}y(t_n) - \frac{1}{3}y(t_{n-1}) + \frac{2h}{3}y'(t_{n+1})$\\
		\item Taylor expand RHS Quantities about time t\\
		$y'(t_{n+1}) = y'(t_n) + hy''(t_n) + \frac{h^2}{2}y'''(t_n) + O(h^3)$\\
		$y(t_{n-1}) = y(t_n) - hy'(t_n) + \frac{h^2}{2}y''(t_n) - \frac{h^3}{6}y'''(t_n) + O(h^4)$\\
		Plugging in we find:\\
		$y_{n+1} = \frac{4}{3}y(t_n) - \frac{1}{3}[y(t_n) - hy'(t_n) + \frac{h^2}{2}y''(t_n) - \frac{h^3}{6}y'''(t_n) + O(h^4)] + \frac{2h}{3}[y'(t_n) + hy''(t_n) + \frac{h^2}{2}y''(t_n) + O(h^3)]$\\
		$= \frac{4}{3}y(t_n) - \frac{1}{3}y(t_n) + \frac{h}{3}y'(t_n) - \frac{h^2}{6}y''(t_n) + \frac{h^3}{18}y'''(t_n) + O(h^4) + \frac{2}{3}hy'(t_n) + \frac{2}{3}h^2y''(t_n) + \frac{2h^3}{6}y'''(t_n) + O(h^4)$\\
		$y_{n+1} = y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + \frac{7}{18}h^3y'''(t_n) + O(h^4)$
		\item True series is:\\
		$y(t_{n+1}) = y(t_n) + hy'(t_n) + \frac{h^2}{2}y''(t_n) + \frac{h^3}{6}y'''(t_n) + O(h^4)$\\
		\item $LTE = y(t_{n+1}) - y_{n+1}$\\
		$= [\frac{1}{6} - \frac{7}{18}]h^3y'''(t_n) + O(h^4)$\\
		$= O(h^3)$
	\end{enumerate}
	
	
	
\end{document}
